{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hachidaishu Vocabulary Dataset Examples\n",
    "\n",
    "This notebook showcases how one might use the Hachidaishu vocabulary dataset from Python. It is available from the Github repository below or through [Google Colab](https://colab.research.google.com/drive/1rS2lbD2rLGw3XxuOroKeEOeWF3MMMTvP?usp=sharing).\n",
    "\n",
    "The Hachidaishu vocabulary dataset is available from Zenodo and Github:\n",
    "\n",
    "-   https://zenodo.org/record/4744170\n",
    "-   https://github.com/yamagen/hachidaishu\n",
    "\n",
    "We will download the dataset from Zenodo or Github and save it as `hachidai.db`. You only need to do this once, unless running under Google Colab or other ephemeral environments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#!wget -c https://zenodo.org/record/4744170/files/hachidai.db?download=1 -O hachidai.db\n",
    "!wget -c https://github.com/yamagen/hachidaishu/raw/main/hachidai.db"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the dataset\n",
    "\n",
    "As we can see in the cell below, the dataset contains space-delimited lines containing the following information:\n",
    "\n",
    "1.  \"01:000001:0007\" consists of 3 fields: 1) anthology, 2) number of poem, and 3) serial ID of the token. The anthology ID indicates respectively: 01..Kokinshu, 02..Gosenshu, 03..Shuishu, 04..Goshuishu, 05..Kin'yoshu, 06..Shikashu, 07..Senzaishu, and 08..Shinkokinshu.\n",
    "2.  Indicates the type of token: A type is a single token; B type is a compound token; C type is a breakdown of B type. A00 indicates a single token; A01 indicates a single token and has another meaning; B00 indicates a compound token; B01 indicates a compound token which has another meaning; C00 indicates the first element of the B00/B01.. breakdown; C01 indicates the second element of the B00/B01.. breakdown.\n",
    "3.  \"BG-02-1527-01-0102\": classification ID based on semantic categories according to Bunruigoihyo (Yamazaki et al. 2014).\n",
    "4.  Indicates the Chasen POS number.\n",
    "5.  Indicates surface form: a form appears in literary works.\n",
    "6.  Indicates lemma in kanji writing.\n",
    "7.  Indicates lemma in kana writing.\n",
    "8.  Indicates conjugated form in kanji writing form.\n",
    "9.  Indicates conjugated form in kana writing form.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!head hachidai.db"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is entirely possible to use standard UNIX command line tools to perform various analyses on the dataset. In the example below, we can extract and count all birds occuring in the dataset in one line. We use the classification ID based on semantic categories (Bunruigohyo) of `BG-01-5620` to filter on the bird semantic category, print the lemma column and tabulate all occurrences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!grep BG-01-5620 hachidai.db | grep 'A00'| awk '{print $6}'| sort | uniq -c | sort -nr | nl "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping ChaSen id to Japanese POS name\n",
    "\n",
    "The cells below contain a mapping from ChaSen's (IPADic's) POS tagset ids to their Japanese names.\n",
    "\n",
    "(This cell is hidden by default, but can be viewed if wanting to see the mappings. Alternatively, you can evaluate the dictionary `chasenid2pos` to see all Japanese mappings in a new cell, or the English version `chasenid2pos_en` to see the mappings in English ([source](https://www.sketchengine.eu/japanese-tagset/)).)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "chasenid2pos = {\n",
    " '00': '？',\n",
    " '01': '名詞',\n",
    " '02': '名詞-一般',\n",
    " '03': '名詞-固有名詞',\n",
    " '04': '名詞-固有名詞-一般',\n",
    " '05': '名詞-固有名詞-人名',\n",
    " '06': '名詞-固有名詞-人名-一般',\n",
    " '07': '名詞-固有名詞-人名-姓',\n",
    " '08': '名詞-固有名詞-人名-名',\n",
    " '09': '名詞-固有名詞-組織',\n",
    " '10': '名詞-固有名詞-地域',\n",
    " '11': '名詞-固有名詞-地域-一般',\n",
    " '12': '名詞-固有名詞-地域-国',\n",
    " '13': '名詞-代名詞',\n",
    " '14': '名詞-代名詞-一般',\n",
    " '15': '名詞-代名詞-縮約',\n",
    " '16': '名詞-副詞可能',\n",
    " '17': '名詞-サ変接続',\n",
    " '18': '名詞-形容動詞語幹',\n",
    " '19': '名詞-数',\n",
    " '20': '名詞-非自立',\n",
    " '21': '名詞-非自立-一般',\n",
    " '22': '名詞-非自立-副詞可能',\n",
    " '23': '名詞-非自立-助動詞語幹',\n",
    " '24': '名詞-非自立-形容動詞語幹',\n",
    " '25': '名詞-特殊-',\n",
    " '26': '名詞-特殊-助動詞語幹',\n",
    " '27': '名詞-接尾',\n",
    " '28': '名詞-接尾-一般',\n",
    " '29': '名詞-接尾-人名',\n",
    " '30': '名詞-接尾-地域',\n",
    " '31': '名詞-接尾-サ変接続',\n",
    " '32': '名詞-接尾-助動詞語幹',\n",
    " '33': '名詞-接尾-形容動詞語幹',\n",
    " '34': '名詞-接尾-副詞可能',\n",
    " '35': '名詞-接尾-助数詞',\n",
    " '36': '名詞-接尾-特殊',\n",
    " '37': '名詞-接続詞的',\n",
    " '38': '名詞-動詞非自立的',\n",
    " '39': '名詞-引用文字列',\n",
    " '40': '名詞-ナイ形容詞語幹',\n",
    " '41': '接頭詞',\n",
    " '42': '接頭詞-名詞接続',\n",
    " '43': '接頭詞-動詞接続',\n",
    " '44': '接頭詞-形容詞接続',\n",
    " '45': '接頭詞-数接続',\n",
    " '46': '動詞',\n",
    " '47': '動詞-自立',\n",
    " '48': '動詞-非自立',\n",
    " '49': '動詞-接尾',\n",
    " '50': '形容詞',\n",
    " '51': '形容詞-自立',\n",
    " '52': '形容詞-非自立',\n",
    " '53': '形容詞-接尾',\n",
    " '54': '副詞',\n",
    " '55': '副詞-一般',\n",
    " '56': '副詞-助詞類接続',\n",
    " '57': '連体詞',\n",
    " '58': '接続詞',\n",
    " '59': '助詞',\n",
    " '60': '助詞-格助詞',\n",
    " '61': '助詞-格助詞-一般',\n",
    " '62': '助詞-格助詞-引用',\n",
    " '63': '助詞-格助詞-連語',\n",
    " '64': '助詞-接続助詞',\n",
    " '65': '助詞-係助詞',\n",
    " '66': '助詞-副助詞',\n",
    " '67': '助詞-間投助詞',\n",
    " '68': '助詞-並立助詞',\n",
    " '69': '助詞-終助詞',\n",
    " '70': '助詞-副助詞／並立助詞／終助詞',\n",
    " '71': '助詞-連体化',\n",
    " '72': '助詞-副詞化',\n",
    " '73': '助詞-特殊',\n",
    " '74': '助動詞',\n",
    " '75': '感動詞',\n",
    " '76': '記号',\n",
    " '77': '記号-一般',\n",
    " '78': '記号-句点',\n",
    " '79': '記号-読点',\n",
    " '80': '記号-空白',\n",
    " '81': '記号-アルファベット',\n",
    " '82': '記号-括弧開',\n",
    " '83': '記号-括弧閉',\n",
    " '84': 'その他',\n",
    " '85': 'その他-間投',\n",
    " '86': 'フィラー',\n",
    " '87': '非言語音',\n",
    " '88': '語断片',\n",
    " '89': '未知語'\n",
    " }\n",
    "\n",
    "chasenid2pos_en = {\n",
    " '00': '?',\n",
    " '01': 'N',\n",
    " '02': 'N.g',\n",
    " '03': 'N.Prop',\n",
    " '04': 'N.Prop.g',\n",
    " '05': 'N.Prop.n',\n",
    " '06': 'N.Prop.n.g',\n",
    " '07': 'N.Prop.n.s',\n",
    " '08': 'N.Prop.n.f',\n",
    " '09': 'N.Prop.o',\n",
    " '10': 'N.Prop.p',\n",
    " '11': 'N.Prop.p.g',\n",
    " '12': 'N.Prop.p.c',\n",
    " '13': 'N.Pron',\n",
    " '14': 'N.Pron.g',\n",
    " '15': 'N.Pron.sh',\n",
    " '16': 'N.Adv',\n",
    " '17': 'N.Vs',\n",
    " '18': 'N.Ana',\n",
    " '19': 'N.Num',\n",
    " '20': 'N.bnd',\n",
    " '21': 'N.bnd.g',\n",
    " '22': 'N.bnd.Adv',\n",
    " '23': 'N.bnd.Aux',\n",
    " '24': 'N.bnd.Ana',\n",
    " '25': 'N.spec',\n",
    " '26': 'N.spec.Aux',\n",
    " '27': 'N.Suff',\n",
    " '28': 'N.Suff.g',\n",
    " '29': 'N.Suff.n',\n",
    " '30': 'N.Suff.p',\n",
    " '31': 'N.Suff.Vs',\n",
    " '32': 'N.Suff.Aux',\n",
    " '33': 'N.Suff.Ana',\n",
    " '34': 'N.Suff.Adv',\n",
    " '35': 'N.Suff.msr',\n",
    " '36': 'N.Suff.spec',\n",
    " '37': 'N.Conj',\n",
    " '38': 'N.V.bnd',\n",
    " '39': 'N.Phr',\n",
    " '40': 'N.nai',\n",
    " '41': 'Pref',\n",
    " '42': 'Pref.N',\n",
    " '43': 'Pref.V',\n",
    " '44': 'Pref.Ai',\n",
    " '45': 'Pref.Num',\n",
    " '46': 'V',\n",
    " '47': 'V.free',\n",
    " '48': 'V.bnd',\n",
    " '49': 'V.Suff',\n",
    " '50': 'Ai',\n",
    " '51': 'Ai.free',\n",
    " '52': 'Ai.bnd',\n",
    " '53': 'Ai.Suff',\n",
    " '54': 'Adv',\n",
    " '55': 'Adv.g',\n",
    " '56': 'Adv.P',\n",
    " '57': 'Adn',\n",
    " '58': 'Conj',\n",
    " '59': 'P',\n",
    " '60': 'P.c',\n",
    " '61': 'P.c.g',\n",
    " '62': 'P.c.r',\n",
    " '63': 'P.c.Phr',\n",
    " '64': 'P.Conj',\n",
    " '65': 'P.bind',\n",
    " '66': 'P.Adv',\n",
    " '67': 'P.ind',\n",
    " '68': 'P.coord',\n",
    " '69': 'P.fin',\n",
    " '70': 'P.advcoordfin',\n",
    " '71': 'P.prenom',\n",
    " '72': 'P.advzer',\n",
    " '73': 'P.spec',\n",
    " '74': 'Aux',\n",
    " '75': 'Interj',\n",
    " '76': 'Sym',\n",
    " '77': 'Sym.g',\n",
    " '78': 'Sym.p',\n",
    " '79': 'Sym.c',\n",
    " '80': 'Sym.w',\n",
    " '81': 'Sym.a',\n",
    " '82': 'Sym.bo',\n",
    " '83': 'Sym.bc',\n",
    " '84': 'Other',\n",
    " '85': 'Other.indir',\n",
    " '86': 'Fill',\n",
    " '87': 'Nss',\n",
    " '88': 'Frgm',\n",
    " '89': 'Unknown'\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python data loader classes\n",
    "\n",
    "Below is an example of how to read the dataset into Python. Several conveniant methods to access the dataset for common analyses are provided:\n",
    "\n",
    "-   `Token` is a Python dataclass containing all information on a token.\n",
    "-   `HachidaishuRecord` is a Python dataclass that provides a wrapper over each line of the `hachidai.db`.\n",
    "-   `HachidaishuDB` is a Python dataclass that wraps over the list of `HachidaishuRecord`s and provides an iterator interface and conveniance methods:\n",
    "    -   `query()`: Iterates over the dataset, optionally filtering for a specific anthology, poem or serial.\n",
    "    -   `tokens()`: Wraps `query()` to return a sequence of tokens, taking into account the preffered tokenization.\n",
    "    -   `text()`: Wraps `query()` to return poems as plain text, one per line, optionally separated by spaces or other delimiters.\n",
    "\n",
    "Note that while there are no methods for filtering on a specific POS tag or similar, this can be easily performed using standard Python functions (an example is given further below)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dataclasses import dataclass, field, fields, astuple, asdict\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "Anthology = Enum('Anthology', 'Kokinshu Gosenshu Shuishu Goshuishu Kin’yoshu Shikashu Senzaishu Shinkokinshu')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    token_type: str\n",
    "    bg_id: str\n",
    "    chasen_id: str = field(repr=False)\n",
    "    pos: str = field(init=False)\n",
    "    surface: str\n",
    "    lemma: str\n",
    "    lemma_reading: str\n",
    "    kanji: str\n",
    "    kanji_reading: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pos = chasenid2pos[self.chasen_id]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.surface}/{self.pos}/{self.lemma}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(astuple(self))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HachidaishuRecord:\n",
    "    anthology: Anthology\n",
    "    poem: int\n",
    "    serial: int\n",
    "    tokens: List[Token]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(astuple(self)[:3] + astuple(self.token()))\n",
    "\n",
    "    def keys(self):\n",
    "        return [f.name for f in fields(self)][:3] + [f.name for f in fields(self.token())]\n",
    "\n",
    "    def token(self):\n",
    "        '''Return the token, ignoring any decomposition or alternative variants.'''\n",
    "        return self.tokens[0]\n",
    "\n",
    "    def decomposition(self):\n",
    "        '''Return a list containing the alternative decomposition of token in record.'''\n",
    "        if len(self.tokens) > 1:\n",
    "            return self.tokens[1:]\n",
    "        else:\n",
    "            return self.tokens\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HachidaishuDB:\n",
    "    db: List[HachidaishuRecord] = field(repr=False)\n",
    "\n",
    "    def __init__(self, filename='hachidai.db'):\n",
    "        self.db = self._retokenize(self._read_db(filename))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.db[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for record in self.db:\n",
    "            yield record\n",
    "\n",
    "    def columns(self):\n",
    "        return self.db[0].keys()\n",
    "\n",
    "    def _retokenize(self, db):\n",
    "        tokens = []\n",
    "        serial = None\n",
    "        for entry in db:\n",
    "            if serial == entry.serial:\n",
    "                tokens[-1].tokens += entry.tokens\n",
    "            else:\n",
    "                tokens.append(entry)\n",
    "            \n",
    "            serial = entry.serial\n",
    "        return tokens\n",
    "\n",
    "    def _read_db(self, filename='hachidai.db'):\n",
    "        with open(filename) as f:\n",
    "            for row in f.readlines():\n",
    "                fields = row.rstrip().split(' ')\n",
    "                id, token_type, bg_id, chasen_id, surface, lemma, lemma_reading, kanji, kanji_reading = fields\n",
    "                anthology, poem, serial = id.split(':')\n",
    "                token = Token(token_type, bg_id, chasen_id, surface,\n",
    "                              lemma, lemma_reading, kanji, kanji_reading)\n",
    "                yield HachidaishuRecord(\n",
    "                    Anthology(int(anthology)),\n",
    "                    int(poem),\n",
    "                    int(serial),\n",
    "                    [token]\n",
    "                    )\n",
    "\n",
    "    def query(self, anthology=None, poem=None, serial=None):\n",
    "        for record in self:\n",
    "            if anthology and record.anthology != Anthology(anthology):\n",
    "                continue\n",
    "            if poem and record.poem != poem:\n",
    "                continue\n",
    "            if serial and record.serial != serial:\n",
    "                continue\n",
    "            yield record\n",
    "\n",
    "    \n",
    "    def tokens(self, mode='default', anthology=None, poem=None, serial=None):\n",
    "        for record in self.query(anthology, poem, serial):\n",
    "            if mode == 'default':\n",
    "                yield record.token()\n",
    "            elif mode == 'decomposition':\n",
    "                for token in record.decomposition():\n",
    "                    yield token\n",
    "    \n",
    "    def text(self, delimiter=' ', anthology=None, poem=None, serial=None):\n",
    "        by_poem = groupby(self.query(anthology=anthology, poem=poem, serial=serial),\n",
    "                          key=lambda r: (r.anthology, r.poem))\n",
    "        return '\\n'.join(delimiter.join(record.token().surface for record in poem)\n",
    "                         for poem_info, poem in by_poem)\n",
    "\n",
    "\n",
    "db = HachidaishuDB()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple usage\n",
    "\n",
    "Let's print the first poem from the Kokinshu, using `/` as a delimiter between tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db.text(delimiter='/', anthology=1, poem=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see more information (such as alternative tokenizations) by doing a query with the same anthology and poem parameters:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "list(db.query(anthology=1, poem=1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, similary, just get all the tokens:\n",
    "(Note you can use the autocomplete feature of your editior to access all antology names by pressing TAB after `Anthology.`, as Anthology is an enum containing the name to id mapping.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "list(db.tokens(anthology=Anthology.Kokinshu, poem=1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the default representation of each token hides everything but the surface form, POS, and lemma form. By writing a custom print function, you can print other pertinent fields."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def token2string(t):\n",
    "    return f'{t.surface},{t.pos},{t.lemma},{t.kanji},{t.bg_id}'\n",
    "\n",
    "list(token2string(token) for token in db.tokens(anthology=Anthology.Kokinshu, poem=1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics\n",
    "\n",
    "Below we can calculate the frequency of POS tags, 20 most common lemmas in the Shinkokinshu, as well as generate an interactive plot, covering the Hachidaishu anthologies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Counter(token.pos for token in db.tokens()).most_common()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Counter(token.lemma for token in db.tokens(anthology=Anthology.Shinkokinshu)).most_common(20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example visualization of POS distribution over anthologies\n",
    "\n",
    "The example uses Pandas and Plotly to plot the distribution of POS tags by anthology."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install pandas plotly"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "dfp = pd.DataFrame.from_records(db.query(), columns=db.columns())\n",
    "dfp.anthology = dfp.anthology.apply(lambda a: f'{a.value}: {a.name}')\n",
    "dfp['pos_1'] = dfp.pos.apply(lambda s: s.split('-')[0])\n",
    "dfp = dfp.groupby(['anthology', 'pos_1']).agg({'pos_1': ['count']})\n",
    "dfp = dfp.reset_index()\n",
    "dfp.columns = ['anthology', 'pos_1', 'count']\n",
    "dfp"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = px.bar(dfp, x=\"anthology\", y=\"count\", color=\"pos_1\", barmode=\"stack\")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas\n",
    "\n",
    "A more straightforward analysis can also be performed using Pandas by treating it as a regular tabular dataset. In this casre, care must be taken when aggregating over tokens by selecting for the chosen token type (variant/decomposition status)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_table('hachidai.db', usecols=range(9), sep=' ',\n",
    "                   names=['id', 'token_type', 'bg_id', 'chasen_id', 'surface', 'lemma', 'lemma_reading', 'kanji', 'kanji_reading'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "birds = df[df.token_type.str.match('A00') & df.bg_id.str.contains('BG-01-5620')]\n",
    "birds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "birds.lemma.value_counts()\n",
    "# These counts should be the same as in our example at the beginning:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
