{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WLSP Hachidaishu Integration\n",
    "\n",
    "The goal of this notebook is to update the WLSP entries in the Hachidaishu dataset from the original to the revised version where possible while also ensuring Hachidaishu-unique entries are safely preserved.\n",
    "This is accomplished by:\n",
    "\n",
    "1.  comparing the original WLSP to the revised version,\n",
    "2.  mapping the Hachidaishu POS tagset (IPAdic) to the UniDic POS tagset,\n",
    "3.  adding UD POS information following current UniDic->UD conventions,\n",
    "4.  mapping tokens to their closest UniDic equivalents where UniDic WLSP entries are available,\n",
    "5.  creating a new dictionary (database) of tokens currently not mappable to UniDic (or new lemma ids per external dataset UniDic support). (WIP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data from several sources:\n",
    "\n",
    "- Hachidaishu database (this repo: `hachidai.db`)\n",
    "- WLSP1 (original WLSP): private data not published due to copyright\n",
    "- WLSP2 (Expanded and revised WLSP--『分類語彙表増補改訂版データベース』（ver.1.0.1）): https://github.com/masayu-a/wlsp\n",
    "- UniDic to WLSP2 mappings: https://github.com/masayu-a/WLSP2UniDic\n",
    "- (Not implemented: UniDic to Historical WLSP mappings: https://github.com/masayu-a/WLSP2UniDic_historical)\n",
    "- UniDic CWJ 3.1 lex file: LZMA compressed lex_3_1.csv from https://ccd.ninjal.ac.jp/unidic_archive/cwj/3.1.0/unidic-cwj-3.1.0.zip\n",
    "\n",
    "Summary table (English version at end of notebook)\n",
    "\n",
    "|     | 名称   | 勅/院宣  | 成立   | 撰者                                                 |   首 |\n",
    "|----:|:-------|:---------|:-------|:-----------------------------------------------------|-----:|\n",
    "|   1 | 古今   | 醍醐天皇 | 905頃  | 紀友則・紀貫之・凡河内躬恒・壬生忠岑                 | 1100 |\n",
    "|   2 | 後撰   | 村上天皇 | 951頃  | 清原元輔・紀時文・大中臣能宣・源順・坂上望城         | 1425 |\n",
    "|   3 | 拾遺   | 花山院   | 1007頃 | 花山院                                               | 1351 |\n",
    "|   4 | 後拾遺 | 白河天皇 | 1086   | 藤原通俊                                             | 1218 |\n",
    "|   5 | 金葉   | 白河院   | 1125頃 | 源俊頼                                               |  665 |\n",
    "|   6 | 詞花   | 崇徳院   | 1151頃 | 藤原顕輔                                             |  415 |\n",
    "|   7 | 千載   | 後白河院 | 1188   | 藤原俊成                                             | 1288 |\n",
    "|   8 | 新古今 | 後鳥羽院 | 1205   | 源通具・藤原有家・藤原定家・藤原家隆・藤原雅経・寂蓮 | 1978 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "old_wlsp_path = Path(\"~/Dropbox/bunrui/\").expanduser() # Not public.\n",
    "new_wlsp_path = Path(\"~/Dropbox/bunrui/wlsp\").expanduser() # From GitHub repo.\n",
    "unidic_wlsp_path = Path(\"~/Dropbox/bunrui/wlsp2unidic\").expanduser() # From GitHub repo.\n",
    "hachidaishu_path = Path(\"~/Dropbox/bunrui/hachidai.db\").expanduser()\n",
    "unidic_lex_path = Path(\n",
    "    \"~/Dropbox/bunrui/lex_3_1.csv.xz\"\n",
    ").expanduser()  # Compressed from lex_3_1.csv in https://ccd.ninjal.ac.jp/unidic_archive/cwj/3.1.0/unidic-cwj-3.1.0.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hachidaishu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hachidaishu import HachidaishuDB, Anthology\n",
    "\n",
    "hachidaishu = HachidaishuDB(filename=\"hachidai.db\")\n",
    "\n",
    "metadata = {\n",
    "    1: {\"no.\": 1, \"name\": \"Kokinshu\", \"name_ja\": \"古今\", \"order_ja\": \"醍醐天皇\", \"order\": \"Daigo tenno\", \"date\": \"ca. 905\", \"editor\": \"紀友則・紀貫之・凡河内躬恒・壬生忠岑\", \"poems\": 1100},\n",
    "    2: {\"no.\": 2, \"name\": \"Gosenshu\", \"name_ja\": \"後撰\", \"order_ja\": \"村上天皇\", \"order\": \"Murakami tenno\", \"date\": \"ca. 951\", \"editor\": \"清原元輔・紀時文・大中臣能宣・源順・坂上望城\", \"poems\": 1425},\n",
    "    3: {\"no.\": 3, \"name\": \"Shuishu\", \"name_ja\": \"拾遺\", \"order_ja\": \"花山院\", \"order\": \"Kazan'in\", \"date\": \"ca. 1007\", \"editor\": \"花山院\", \"poems\": 1351},\n",
    "    4: {\"no.\": 4, \"name\": \"Goshuishu\", \"name_ja\": \"後拾遺\", \"order_ja\": \"白河天皇\", \"order\": \"Shirakawa tenno\", \"date\": \"1086\", \"editor\": \"藤原通俊\", \"poems\": 1218},\n",
    "    5: {\"no.\": 5, \"name\": \"Kin’yoshu\", \"name_ja\": \"金葉\", \"order_ja\": \"白河院\", \"order\": \"Shirakawain\", \"date\": \"ca. 1125\", \"editor\": \"源俊頼\", \"poems\": 665},\n",
    "    6: {\"no.\": 6, \"name\": \"Shikashu\", \"name_ja\": \"詞花\", \"order_ja\": \"崇徳院\", \"order\": \"Sutokuin\", \"date\": \"ca. 1151\", \"editor\": \"藤原顕輔\", \"poems\": 415},\n",
    "    7: {\"no.\": 7, \"name\": \"Senzaishu\", \"name_ja\": \"千載\", \"order_ja\": \"後白河院\", \"order\": \"Goshirakawain\", \"date\": \"1188\", \"editor\": \"藤原俊成\", \"poems\": 1288},\n",
    "    8: {\"no.\": 8, \"name\": \"Shinkokinshu\", \"name_ja\": \"新古今\", \"order_ja\": \"後鳥羽院\", \"order\": \"Gotobain\", \"date\": \"1205\", \"editor\": \"源通具・藤原有家・藤原定家・藤原家隆・藤原雅経・寂蓮\", \"poems\": 1978},\n",
    "}\n",
    "\n",
    "def decode_bg_id(s):\n",
    "    \"\"\"Returns a dictionary containing a 分類番号 key mapped to a WLSP article record string.\n",
    "    CH-JP-0000 are unique to Hachidaishu.\"\"\"\n",
    "    xs = s.split(\"-\")\n",
    "    return {\"分類番号\": f\"{xs[1][1]}.{xs[2]}\"}\n",
    "\n",
    "\n",
    "bgid_hachidaishu = set(decode_bg_id(t.bg_id)[\"分類番号\"] for t in hachidaishu.tokens())\n",
    "len(bgid_hachidaishu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WLSP1 (Original WLSP; WLSPH in TEI/output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(old_wlsp_path / \"sakuin.dat\") as f:\n",
    "    # 段落内番号 => 小段落番号\n",
    "    # 分類番号は合うものの，\"段落番号\", \"小段落番号\"が基本的に合わないのでマッピングする必要あり\n",
    "    reader = csv.DictReader(\n",
    "        f, fieldnames=[\"reading\", \"orth\", \"分類番号\", \"段落番号\", \"小段落番号\", \"info\", \"note\"]\n",
    "    )\n",
    "    wlsp1 = [r for r in reader]\n",
    "    for r in wlsp1:\n",
    "        if len(r[\"分類番号\"]) == 5:\n",
    "            r[\"分類番号\"] += \"0\"\n",
    "        r[\"段落番号\"] = f'{int(r[\"段落番号\"]):02}'\n",
    "\n",
    "# Index: orth -> WLSP1 id\n",
    "wlsp1_index = defaultdict(list)\n",
    "\n",
    "for r in wlsp1:\n",
    "    wlsp1_index[r[\"orth\"]].append(r)\n",
    "\n",
    "# Index: lemma (Hachidaishu only) -> WLSP1 id\n",
    "wlsp1h_index = defaultdict(set)\n",
    "for token in hachidaishu.tokens():\n",
    "    wlsp1h_index[token.lemma].add(decode_bg_id(token.bg_id)[\"分類番号\"])\n",
    "\n",
    "wlsp1h_index\n",
    "\n",
    "len(wlsp1_index), len(wlsp1h_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlsp1[0], wlsp1h_index['や']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WLSP2 (Expanded and revised WLSP; WLSP in TEI/output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_wlsp_path / \"bunruidb.txt\") as f:\n",
    "    reader = csv.DictReader(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"id\",\n",
    "            \"見出し番号\",\n",
    "            \"record_type\",\n",
    "            \"類\",\n",
    "            \"部門\",\n",
    "            \"中項目\",\n",
    "            \"分類項目\",\n",
    "            \"分類番号\",\n",
    "            \"段落番号\",\n",
    "            \"小段落番号\",\n",
    "            \"語番号\",\n",
    "            \"orth_info\",\n",
    "            \"orth\",\n",
    "            \"reading\",\n",
    "            \"reverse_reading\",\n",
    "        ],\n",
    "    )\n",
    "    wlsp2 = [r for r in reader]\n",
    "\n",
    "# Index: orth -> WLSP2 id\n",
    "wlsp2_index = defaultdict(list)\n",
    "\n",
    "for r in wlsp2:\n",
    "    wlsp2_index[r[\"orth\"]].append(r)\n",
    "\n",
    "# Index: lemma (Hachidaishu only) -> WLSP2 id\n",
    "wlsp2h_index = defaultdict(set)\n",
    "for token in hachidaishu.tokens():\n",
    "    wlsp2h_index[token.lemma].add(decode_bg_id(token.bg_id)[\"分類番号\"])\n",
    "\n",
    "len(wlsp2_index), len(wlsp2h_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "wlsp2describe = defaultdict(set)\n",
    "WLSPRecord = namedtuple(\"WLSPRecord\", ['類', '部門', '中項目', '分類項目'])\n",
    "for token, records in wlsp2_index.items():\n",
    "    for record in records:\n",
    "        wlsp2describe[record['分類番号']].add(WLSPRecord(record['類'], record['部門'], record['中項目'], record['分類項目']))\n",
    "\n",
    "for id, rs in wlsp2describe.items():\n",
    "    assert len(rs) == 1\n",
    "    wlsp2describe[id] = list(rs)[0]\n",
    "\n",
    "wlsp2describe = dict(wlsp2describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniDic to WLSP2 mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(unidic_wlsp_path / \"BunruiNo_LemmaID.txt\") as f:\n",
    "    wlsp_number2label = {}\n",
    "    wlsp2unidic_lemma_id = {}\n",
    "    lemma_id2wlsp2 = {}\n",
    "    lines = f.readlines()[1:]\n",
    "    for line in lines:\n",
    "        b, lemma_id = line.rstrip().split(\"\\t\")\n",
    "        number, label, sub_number = b.split(\",\")\n",
    "        # Check if number to label mappings are unique: (✅ they are)\n",
    "        assert number not in wlsp_number2label or label == wlsp_number2label[number]\n",
    "        wlsp_number2label[number] = label\n",
    "        wlsp2unidic_lemma_id[number] = lemma_id\n",
    "        lemma_id2wlsp2[lemma_id] = number\n",
    "\n",
    "len(wlsp_number2label), len(wlsp2unidic_lemma_id), len(lemma_id2wlsp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlsp_number2label[\"1.1010\"], wlsp2unidic_lemma_id[\"1.1010\"], lemma_id2wlsp2[\"65788\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniDic DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import csv\n",
    "from collections import namedtuple  # we want to be able to hash dict entries\n",
    "\n",
    "with lzma.open(unidic_lex_path, \"rt\") as f:\n",
    "    # l1..4 are not used. l1 is pre-NFKC'd orth?\n",
    "    fields = [\n",
    "        \"l1\",\n",
    "        \"l2\",\n",
    "        \"l3\",\n",
    "        \"l4\",\n",
    "        \"pos1\",\n",
    "        \"pos2\",\n",
    "        \"pos3\",\n",
    "        \"pos4\",\n",
    "        \"cType\",\n",
    "        \"cForm\",\n",
    "        \"lForm\",\n",
    "        \"lemma\",\n",
    "        \"orth\",\n",
    "        \"pron\",\n",
    "        \"orthBase\",\n",
    "        \"pronBase\",\n",
    "        \"goshu\",\n",
    "        \"iType\",\n",
    "        \"iForm\",\n",
    "        \"fType\",\n",
    "        \"fForm\",\n",
    "        \"iConType\",\n",
    "        \"fConType\",\n",
    "        \"type\",\n",
    "        \"kana\",\n",
    "        \"kanaBase\",\n",
    "        \"form\",\n",
    "        \"formBase\",\n",
    "        \"aType\",\n",
    "        \"aConType\",\n",
    "        \"aModType\",\n",
    "        \"lid\",\n",
    "        \"lemma_id\",\n",
    "    ]\n",
    "    UniDicRecord = namedtuple(\"UniDicRecord\", fields)\n",
    "    reader = csv.DictReader(f, fieldnames=fields)\n",
    "    unidic_db = [UniDicRecord(**r) for r in reader]\n",
    "\n",
    "# Some useful mapping dicts # FIXME these are not 1:1 (i.e. a lemma string might have more than one entry/id), they should be defaultdict(set)\n",
    "lemma_id2s = {e.lemma_id: e.lemma for e in unidic_db}\n",
    "lemma_s2id = {e.lemma: e.lemma_id for e in unidic_db}\n",
    "\n",
    "# This is an expensive index, but allows quick indexing into all entries from a given orth or lemma token.\n",
    "unidic_token_index = defaultdict(set)\n",
    "for e in unidic_db:\n",
    "    unidic_token_index[e.lemma].add(e)\n",
    "    unidic_token_index[e.orth].add(e)\n",
    "\n",
    "len(unidic_db), len(lemma_id2s), len(lemma_s2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidic_token_index[\"言ふ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Update WLSP mappings\n",
    "\n",
    "In order to map from the old to the new WLSP, where there were small changes to the last digit(s) of the article number, we use a simple distance function.\n",
    "The distance function allows us to match entries of the same token that has slightly different IDs in the two editions.\n",
    "In the future, this should be converted to a WordNet-like synset() functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgid_distance(a, b):\n",
    "    \"\"\"分類語彙表ID aとbを比較し，どの程度一致しているかを0（完全不一致）から5（完全一致）で返す。\n",
    "    体の類（1.X）と用の類（2.X）は最初から異なるため0を返すが，同じ類内では以下のようになる：\n",
    "    >>> a = '1.1610'; b = '1.1600'; bgid_distance(a, b)\n",
    "    3\n",
    "    \"\"\"\n",
    "    for i, (x, y) in enumerate(zip(a, b)):\n",
    "        if x != y:\n",
    "            if i > 0:\n",
    "                return i - 1  # .の分を引く\n",
    "            else:\n",
    "                return i\n",
    "    else:\n",
    "        return len(a) - 1  # .の分を引く\n",
    "\n",
    "\n",
    "bgid_distance(\"1.1610\", \"1.1600\"), bgid_distance(\"1.1610\", \"1.1610\"), bgid_distance(\n",
    "    \"1.1610\", \"1.1611\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations, product\n",
    "\n",
    "token_match_types = {\n",
    "    \"no\": 0,  # WLSP2及びUniDicに該当しない\n",
    "    \"full\": 0, # WLSP2と完全一致\n",
    "    \"unidic_only\": 0, # UniDicのみ該当（WLSP2ではヒットしない）\n",
    "    \"mixed\": 0, # WLSP1と2で多少の項目が異なるが，一致している項目がある\n",
    "    \"partial\": 0, # WLSP1と2で項目が一致しないが，違い項目が少なくとも一つある\n",
    "    \"convergent\": 0, # WLSP1と2で項目が全く一致しない（X.YYYYYのXレベルでの違い）\n",
    "}\n",
    "wlsp1_to_2 = defaultdict(set)\n",
    "token_wlsp1_to_2 = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "for token, token_bgids in wlsp1h_index.items():\n",
    "    if token not in wlsp2_index:\n",
    "        if token in unidic_token_index:\n",
    "            token_match_types[\"unidic_only\"] += 1\n",
    "            for t in unidic_token_index[token]:\n",
    "                for id in token_bgids:\n",
    "                    if t.lemma_id in lemma_id2wlsp2:\n",
    "                        wlsp1_to_2[id].add(lemma_id2wlsp2[t.lemma_id])\n",
    "                        token_wlsp1_to_2[token][id].add(lemma_id2wlsp2[t.lemma_id])\n",
    "        else:\n",
    "            token_match_types[\"no\"] += 1\n",
    "    else:\n",
    "        bgid1 = token_bgids\n",
    "        bgid2 = {e[\"分類番号\"] for e in wlsp2_index[token]}\n",
    "\n",
    "        if bgid1.issubset(bgid2):\n",
    "            token_match_types[\"full\"] += 1\n",
    "            for id1 in token_bgids:\n",
    "                wlsp1_to_2[id1].add(id1)\n",
    "                token_wlsp1_to_2[token][id1].add(id1)\n",
    "                # If we wanted to add all possible mappings instead of keeping the same mapping:\n",
    "                # for e in wlsp2_index[token]:\n",
    "                #     wlsp1_to_2[id1].add(e[\"分類番号\"])\n",
    "        elif len(bgid1.intersection(bgid2)) > 0:\n",
    "            token_match_types[\"mixed\"] += 1\n",
    "            common_ids = bgid1.intersection(bgid2)\n",
    "            for id in common_ids:\n",
    "                wlsp1_to_2[id].add(id)\n",
    "                token_wlsp1_to_2[token][id].add(id)\n",
    "        else:\n",
    "            matches = {(a, b): bgid_distance(a, b) for a, b in product(bgid1, bgid2)}\n",
    "            top_matches = sorted(matches.items(), key=lambda x: x[1], reverse=True)\n",
    "            if not any(v > 0 for k, v in top_matches):\n",
    "                token_match_types[\"convergent\"] += 1\n",
    "            else:\n",
    "                token_match_types[\"partial\"] += 1\n",
    "                # We take the top 1 match for now\n",
    "                top_mapping = top_matches[0][0]\n",
    "                bgid1, bgid2 = top_mapping\n",
    "                wlsp1_to_2[bgid1].add(bgid2)\n",
    "                token_wlsp1_to_2[token][bgid1].add(bgid2)\n",
    "\n",
    "token_match_types, len(wlsp1_to_2), len(wlsp1_to_2)/len(bgid_hachidaishu), wlsp1_to_2['1.1630']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_wlsp1_to_2), token_wlsp1_to_2['来']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "\n",
    "Below is work in progress/temporary workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wlsp2の場合\n",
    "{\n",
    "    \"id\": \"058059\",\n",
    "    \"見出し番号\": \"55858\",\n",
    "    \"record_type\": \"A\",\n",
    "    \"類\": \"体\",\n",
    "    \"部門\": \"自然\",\n",
    "    \"中項目\": \"自然\",\n",
    "    \"分類項目\": \"色\",\n",
    "    \"分類番号\": \"1.5020\",\n",
    "    \"段落番号\": \"12\",\n",
    "    \"小段落番号\": \"01\",\n",
    "    \"語番号\": \"01\",\n",
    "    \"orth_info\": \"青\",\n",
    "    \"orth\": \"青\",\n",
    "    \"reading\": \"あお\",\n",
    "    \"reverse_reading\": \"おあ\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in wlsp1 if r[\"分類番号\"] == \"1.1770\"][0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list((t.bg_id, decode_bg_id(t.bg_id), t) for t in hachidaishu.tokens())[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgid_hachidaishu = set(decode_bg_id(t.bg_id)[\"分類番号\"] for t in hachidaishu.tokens())\n",
    "len(bgid_hachidaishu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "wlsph_index = defaultdict(set)\n",
    "for token in hachidaishu.tokens():\n",
    "    wlsph_index[token.lemma].add(decode_bg_id(token.bg_id)[\"分類番号\"])\n",
    "\n",
    "wlsph_index['や']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(r[\"orth\"] for r in wlsp1)), len(wlsp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in wlsp1 if r[\"orth\"] == \"中\"][:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(r[\"orth\"] for r in wlsp1).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlsp1_tokens = set(r[\"orth\"] for r in wlsp1)\n",
    "wlsp2_tokens = set(r[\"orth\"] for r in wlsp2)\n",
    "wlsph_tokens = set(t for t in wlsph_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wlsp1_tokens), len(wlsp2_tokens), len(wlsp1_tokens.difference(wlsp2_tokens)) / len(\n",
    "    wlsp1_tokens\n",
    "), len(wlsp2_tokens.difference(wlsp1_tokens)) / len(wlsp2_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wlsph_tokens), len(wlsph_tokens.difference(wlsp2_tokens)), list(wlsph_tokens.difference(wlsp2_tokens))[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in hachidaishu[:12]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "ts = list(hachidaishu.tokens())\n",
    "token_count = len(ts)\n",
    "character_count = sum(len(c.surface) for c in ts)\n",
    "anthology_count = len(list(groupby(hachidaishu, lambda r: r.anthology)))\n",
    "poem_count = len(list(groupby(hachidaishu, lambda r: r.poem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEI conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from lxml import objectify as o\n",
    "\n",
    "xml_ns = \"{http://www.w3.org/XML/1998/namespace}\"\n",
    "\n",
    "from datetime import date    \n",
    "today = date.today().isoformat()\n",
    "\n",
    "E = o.ElementMaker(\n",
    "    annotate=False,\n",
    "    namespace=\"http://www.tei-c.org/ns/1.0\",\n",
    "    nsmap={None: \"http://www.tei-c.org/ns/1.0\"},\n",
    ")\n",
    "\n",
    "header = E.teiHeader(\n",
    "    E.fileDesc(\n",
    "        E.titleStmt(\n",
    "            E.title(\"Hachidaishu dataset\"),\n",
    "            E.author(E.persName(E.forename(\"Hilofumi\"), E.surname(\"Yamamoto\"))),\n",
    "            E.author(E.persName(E.forename(\"Bor\"), E.surname(\"Hodošček\"))),\n",
    "        ),\n",
    "        E.editionStmt(\n",
    "            E.edition(f\"{1}st edition\", n=str(1)),\n",
    "            E.respStmt(\n",
    "                E.resp(\"Encoded by\"),\n",
    "                E.persName(E.forename(\"Bor\"), E.surname(\"Hodošček\"))\n",
    "            )\n",
    "        ),\n",
    "        E.extent(\n",
    "            E.measure(f\"{character_count:,} characters\", unit=\"characters\", quantity=f\"{character_count}\"),\n",
    "            E.measure(f\"{token_count:,} morphemes\", unit=\"morphemes\", quantity=f\"{token_count}\"),\n",
    "            E.measure(f\"{poem_count:,} poems\", unit=\"poems\", quantity=f\"{poem_count}\"),\n",
    "            E.measure(f\"{anthology_count:,} anthologies\", unit=\"anthologies\", quantity=f\"{anthology_count}\"),\n",
    "        ),\n",
    "        E.publicationStmt(\n",
    "            E.publisher(\"Bor Hodošček and Hilofumi Yamamoto\"),\n",
    "            # E.distributor(),\n",
    "            # E.idno(),\n",
    "            E.availability(\n",
    "                E.licence(\n",
    "                    E.ab(\"CC BY-SA 4.0\",\n",
    "                    E.ref(\" Licence\", target=\"https://creativecommons.org/licenses/by-sa/4.0/\")) \n",
    "                )\n",
    "            ),\n",
    "            E.date(when=today),\n",
    "        ),\n",
    "        E.sourceDesc(\n",
    "            E.listBibl(\n",
    "                E.head(\"Works consulted in creating the original Hachidaishu database.\"),\n",
    "                E.bibl(\"「新編国歌大観CD-ROM版」（1996）『新編国歌大観』編集委員会監修\"),\n",
    "                E.bibl(\"中村他（1999）「国文学研究資料館編集二十一代集データベース」\"), # https://www.iwanami.co.jp/book/b266286.html\n",
    "                E.bibl(\"新日本古典文学大系本二十一代集\"),\n",
    "                E.bibl(\"久保田（1979）『新潮日本古典集成の新古今集』\"),\n",
    "                E.bibl(\"ヴァージニア大学日本語テキストイニシアティブ監修\"),\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    E.encodingDesc(\n",
    "        E.projectDesc(\n",
    "            E.p(\"\"\"This is a conversion of the space-delimited database format of the Hachidaishu dataset into TEI format. The original Chasen IPAdic POS tags were automatically converted into UniDic POS tags, then into Universal Dependencies POS tags. Word List by Semantic Principle (WLSP) entries were (partially) updated from the floppy disk edition to the newest 1.1 version.\"\"\"),\n",
    "        ),\n",
    "        E.classDecl(\n",
    "            E.taxonomy({f\"{xml_ns}id\": \"NDC\"},\n",
    "                E.bibl(\n",
    "                    E.title(\"Nippon Decimal Classification\"),\n",
    "                    E.edition(\"9\"), \n",
    "                    E.ptr(target=\"https://ndc.datasearch.jp/\")\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    E.profileDesc(\n",
    "        E.langUsage(\n",
    "            E.language(\"Japanese (ca. 905-1205)\", ident=\"ja\")\n",
    "        ),\n",
    "        E.textClass(\n",
    "            E.classCode(\"911\", scheme=\"#NDC\"),\n",
    "            E.classCode(\"Q30038136\", scheme=\"http://www.wikidata.org/entity/\")\n",
    "        ),\n",
    "        # FIXME\n",
    "        # E.textDesc(\n",
    "        #     E.purpose(type=\"\", degree=\"\"),\n",
    "        #     E.channel(mode=\"w\"),\n",
    "        #     n=\"Waka poetry\",\n",
    "        # )\n",
    "    ),\n",
    "    E.revisionDesc(\n",
    "        E.listChange( # TODO Get from git\n",
    "            E.change(\"upload to repo\", when=today, who=\"Bor Hodošček\"),\n",
    "        ),\n",
    "        status=\"published\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"Formats token following CONLL-U UD conventions for feature description.\n",
    "    In the future, this should be rather rendered into XML tags.\"\"\"\n",
    "    wlsp1 = decode_bg_id(token.bg_id)['分類番号']\n",
    "    wlsp2s = token_wlsp1_to_2[token.lemma]\n",
    "    wlsp2 = None\n",
    "    if wlsp1 in wlsp2s:\n",
    "        if wlsp1 in wlsp2s[wlsp1]:\n",
    "            wlsp2 = wlsp1\n",
    "        else:\n",
    "            wlsp2 = list(wlsp2s)[0]\n",
    "    elif len(wlsp2s) > 0:\n",
    "        wlsp2 = list(wlsp2s)[0]\n",
    "    else:\n",
    "        wlsp2 = \"UNK\"\n",
    "\n",
    "    try:\n",
    "        wd = wlsp2describe[wlsp2]\n",
    "        description = f\"{wd.類}-{wd.部門}-{wd.中項目}-{wd.分類項目}\"\n",
    "    except:\n",
    "        description = \"UNK\"\n",
    "        \n",
    "    if wlsp2 == \"UNK\":\n",
    "        wlsp2 = \"\"\n",
    "    else:\n",
    "        wlsp2 = f\"|WLSP={wlsp2}\"\n",
    "\n",
    "    # Makes WLSP description optional when unknown/unmapped.\n",
    "    if description == \"UNK\":\n",
    "        description = \"\"\n",
    "    else:\n",
    "        description = f\"|WLSPDescription={description}\"\n",
    "\n",
    "    return f\"UPosTag={token.ud_pos}|IPAPosTag={token.ipa_pos}|UniDicPosTag={token.unidic_pos}|LemmaReading={token.lemma_reading}|Kanji={token.kanji}|KanjiReading={token.kanji_reading}|WLSPH={wlsp1}{wlsp2}{description}\"\n",
    "\n",
    "def generate_body(db):\n",
    "    body = E.body()\n",
    "    for anthology, poems in groupby(db, key=lambda r: r.anthology):\n",
    "        anthology_div = o.SubElement(body, \"div\", type=\"anthology\", n=anthology.name)\n",
    "        for poem, xs in groupby(poems, key=lambda r: r.poem):\n",
    "            # TODO app/rdg[@wit]\n",
    "            lg = o.SubElement(anthology_div, \"lg\", type=\"waka\", n=str(poem))\n",
    "            l = o.SubElement(lg, \"l\")\n",
    "            for segment in xs:\n",
    "                decompositions = segment.decompositions()\n",
    "                if len(decompositions) == 1: # no variants\n",
    "                    t = decompositions[0][0]\n",
    "                    w = o.SubElement(l, \"w\", pos=t.ipa_en_pos, lemma=t.lemma, msd=format_token(t))\n",
    "                    w._setText(t.surface)\n",
    "                else:\n",
    "                    app = o.SubElement(l, \"app\")\n",
    "                    for decomposition in decompositions:\n",
    "                        rdg = o.SubElement(app, \"rdg\")\n",
    "                        for token in decomposition.tokens:\n",
    "                            w = o.SubElement(rdg, \"w\", pos=token.ipa_en_pos, lemma=token.lemma, msd=format_token(token))\n",
    "                            w._setText(token.surface)\n",
    "    return body\n",
    "\n",
    "text = E.text(generate_body(hachidaishu))\n",
    "\n",
    "root = E.TEI(header, text, {f\"{xml_ns}lang\": \"ja\"})\n",
    "\n",
    "# print(etree.tostring(root, pretty_print=True, encoding=\"unicode\"))\n",
    "\n",
    "with open(\"hachidaishu.xml\", \"w\") as f:\n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    tei_string = etree.tostring(root, pretty_print=True, encoding=\"unicode\")\n",
    "    f.write(tei_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://adrien.barbaresi.eu/blog/validating-tei-xml-python.html\n",
    "!wget -c https://tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng\n",
    "with open(\"tei_all.rng\") as f:\n",
    "    schema = f.read()\n",
    "    schema = schema.replace('<?xml version=\"1.0\" encoding=\"utf-8\"?>', '<?xml version=\"1.0\"?>', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "relaxng_doc = etree.parse(StringIO(schema))\n",
    "tei_relaxng = etree.RelaxNG(relaxng_doc)\n",
    "mytree = etree.parse(\"hachidaishu.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = tei_relaxng.assert_(mytree)\n",
    "    print(\"Valid.\")\n",
    "except AssertionError as err:\n",
    "    print(\"TEI validation error:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_points = len(root.xpath(\"//app\"))\n",
    "variation_segments = len(root.xpath(\"//rdg\"))\n",
    "all_tokens = len(root.xpath(\"//w\"))\n",
    "xml_tokens = len(root.xpath(\"//l/w | //l/app\"))\n",
    "tokens = len(list(hachidaishu.tokens()))\n",
    "poems = len(root.xpath(\"//l\"))\n",
    "lg_poems = len(root.xpath(\"//lg\"))\n",
    "num_anthologies = len(root.xpath(\"//div[@type='anthology']\"))\n",
    "\n",
    "assert poems == lg_poems\n",
    "assert xml_tokens == tokens\n",
    "\n",
    "num_anthologies, poems, variation_points, variation_segments, all_tokens, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msd_to_json(s):\n",
    "    json = {}\n",
    "    records = s.split(\"|\")\n",
    "    for record in records:\n",
    "        k, v = record.split(\"=\")\n",
    "        json[k] = v\n",
    "    return json\n",
    "\n",
    "flat_records = []\n",
    "for anthology in root.xpath(\"//div[@type='anthology']\"):\n",
    "    for poem in anthology.xpath(\"lg\"):\n",
    "        for l in poem.iter(\"l\"):\n",
    "            for token in l.iterchildren():\n",
    "                if token.tag == \"app\":\n",
    "                    token_record = list(list(token.iterchildren())[0].iterchildren())[0]\n",
    "                else:\n",
    "                    token_record = token\n",
    "                flat_records.append({\"Anthology\": anthology.attrib[\"n\"],\n",
    "                                     \"Poem\": poem.attrib[\"n\"],\n",
    "                                     \"Surface\": token_record.text,\n",
    "                                     \"Lemma\": token_record.attrib[\"lemma\"],\n",
    "                                     \"POS\": token_record.attrib[\"pos\"]} | msd_to_json(token_record.attrib[\"msd\"]))\n",
    "\n",
    "import json\n",
    "with open(\"hachidaishu.jsonl\", \"w\") as f:\n",
    "    for r in flat_records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table from beginning\n",
    "import polars as pl\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(40)\n",
    "pl.DataFrame([v for k, v in metadata.items()]).with_columns([\n",
    "    pl.col(\"no.\"), pl.col(\"name\").cast(pl.Categorical), pl.col(\"order\").cast(pl.Categorical), pl.col(\"date\"), pl.col(\"poems\")\n",
    "]).select([\"no.\", \"name\", \"order\", \"date\", \"poems\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
